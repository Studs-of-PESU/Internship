{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hand mediapipe\n",
    "\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# import mediapipe as mp\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Load the trained CNN model\n",
    "# model = load_model('sign_language_model.keras')\n",
    "\n",
    "# # Load label encoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# label_encoder.classes_ = np.load('label_encoder_classes.npy', allow_pickle=True)\n",
    "\n",
    "# # MediaPipe Hands setup\n",
    "# mp_hands = mp.solutions.hands\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "# hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7)\n",
    "\n",
    "# # Function to predict sign language word from keypoints\n",
    "# def predict_sign_language(keypoints):\n",
    "#     keypoints = np.array(keypoints)\n",
    "#     keypoints = keypoints.reshape(1, len(keypoints), 1)\n",
    "#     prediction = model.predict(keypoints)\n",
    "#     label_index = np.argmax(prediction)\n",
    "#     predicted_label = label_encoder.inverse_transform([label_index])[0]\n",
    "#     return predicted_label\n",
    "\n",
    "# # Function to process each frame from the camera feed\n",
    "# def process_frame(frame):\n",
    "#     # Convert the BGR image to RGB\n",
    "#     image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     # Flip the image horizontally for a later selfie-view display\n",
    "#     image_rgb = cv2.flip(image_rgb, 1)\n",
    "#     # To improve performance, optionally mark the image as not writeable to pass by reference\n",
    "#     image_rgb.flags.writeable = False\n",
    "#     # Process the image\n",
    "#     results = hands.process(image_rgb)\n",
    "#     # Draw the hand landmarks on the image\n",
    "#     image_rgb.flags.writeable = True\n",
    "#     image_rgb = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "#     if results.multi_hand_landmarks:\n",
    "#         for hand_landmarks in results.multi_hand_landmarks:\n",
    "#             mp_drawing.draw_landmarks(\n",
    "#                 image_rgb, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "#             # Extract keypoints\n",
    "#             keypoints = []\n",
    "#             for landmark in hand_landmarks.landmark:\n",
    "#                 keypoints.append(landmark.x)\n",
    "#                 keypoints.append(landmark.y)\n",
    "#                 keypoints.append(landmark.z if landmark.z else 0)  # Some landmarks might not have z\n",
    "#             # Predict sign language word\n",
    "#             predicted_label = predict_sign_language(keypoints)\n",
    "#             # Display predicted label\n",
    "#             cv2.putText(image_rgb, f'Predicted: {predicted_label}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "#     return image_rgb\n",
    "\n",
    "# # Main function to capture video from camera\n",
    "# def main():\n",
    "#     cap = cv2.VideoCapture(0)\n",
    "#     while cap.isOpened():\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             print(\"Ignoring empty camera frame.\")\n",
    "#             continue\n",
    "        \n",
    "#         # Process each frame\n",
    "#         processed_frame = process_frame(frame)\n",
    "        \n",
    "#         # Display the resulting frame\n",
    "#         cv2.imshow('Sign Language Recognition', processed_frame)\n",
    "        \n",
    "#         # Exit the loop if the 'q' key is pressed\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "\n",
    "#     # Release the capture object and destroy all windows\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# two hands mediapipe\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the trained CNN model\n",
    "model = load_model('sign_language_model.keras')\n",
    "\n",
    "# Load label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = np.load('label_encoder_classes.npy', allow_pickle=True)\n",
    "\n",
    "# MediaPipe Hands setup\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.7)\n",
    "\n",
    "# Function to predict sign language word from keypoints\n",
    "def predict_sign_language(keypoints):\n",
    "    keypoints = np.array(keypoints)\n",
    "    keypoints = keypoints.reshape(1, len(keypoints), 1)\n",
    "    prediction = model.predict(keypoints)\n",
    "    label_index = np.argmax(prediction)\n",
    "    predicted_label = label_encoder.inverse_transform([label_index])[0]\n",
    "    return predicted_label\n",
    "\n",
    "# Function to process each frame from the camera feed\n",
    "def process_frame(frame):\n",
    "    # Convert the BGR image to RGB\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # Flip the image horizontally for a later selfie-view display\n",
    "    image_rgb = cv2.flip(image_rgb, 1)\n",
    "    # To improve performance, optionally mark the image as not writeable to pass by reference\n",
    "    image_rgb.flags.writeable = False\n",
    "    # Process the image\n",
    "    results = hands.process(image_rgb)\n",
    "    # Draw the hand landmarks on the image\n",
    "    image_rgb.flags.writeable = True\n",
    "    image_rgb = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image_rgb, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            # Extract keypoints\n",
    "            keypoints = []\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                keypoints.append(landmark.x)\n",
    "                keypoints.append(landmark.y)\n",
    "                keypoints.append(landmark.z if landmark.z else 0)  # Some landmarks might not have z\n",
    "            # Predict sign language word\n",
    "            predicted_label = predict_sign_language(keypoints)\n",
    "            # Display predicted label\n",
    "            cv2.putText(image_rgb, f'Predicted: {predicted_label}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    return image_rgb\n",
    "\n",
    "# Main function to capture video from camera\n",
    "def main():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "        \n",
    "        # Process each frame\n",
    "        processed_frame = process_frame(frame)\n",
    "        \n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Sign Language Recognition', processed_frame)\n",
    "        \n",
    "        # Exit the loop if the 'q' key is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the capture object and destroy all windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
